<!DOCTYPE html><html lang="en" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon_io/apple-touch-icon.png"><link rel="icon" href="/img/favicon_io/favicon.ico"><link rel="canonical" href="https://mundi-xu.github.io/2025/02/14/Deepseek-Technical-Principle-Explanation-and-Model-Security-Risk-Assessment/"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"><meta name="theme-color" content="#2f4154"><meta name="author" content="煊宇"><meta name="keywords" content="LLM Security"><meta property="article:published_time" content="2025-02-14T12:05:21.000Z"><meta property="article:modified_time" content="2025-02-17T13:05:10.000Z"><meta property="article:section" content="LLM Security"><meta property="article:tag" content="LLM Security"><meta property="article:tag" content="Threat Modeling"><meta property="article:tag" content="DeepSeek"><meta property="article:tag" content="Model Safety"><meta name="google-site-verification" content="8weHOmi2lqvnOxDE30WJFT51umo63nyCgfm8dXHNT5g"><meta name="robots" content="index,follow"><meta name="googlebot" content="index,follow"><link rel="dns-prefetch" href="//at.alicdn.com"><link rel="dns-prefetch" href="//cdnjs.cloudflare.com"><link rel="dns-prefetch" href="//raw.githubusercontent.com"><link rel="dns-prefetch" href="//busuanzi.ibruce.info"><link rel="preconnect" href="https://at.alicdn.com" crossorigin><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin><link rel="preconnect" href="https://busuanzi.ibruce.info" crossorigin><link rel="dns-prefetch" href="//www.googletagmanager.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin><link rel="preload" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css" as="style" onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"></noscript><link rel="preload" href="/css/main.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/events.js" as="script"><link rel="preload" href="/js/plugins.js" as="script"><link rel="preload" href="/js/boot.js" as="script"><link rel="preload" href="/js/img-lazyload.js" as="script"><meta name="description" content="深度解析DeepSeek V3和R1的核心技术原理，包括MLA架构优化、MoE专家混合、FP8混合精度训练等创新技术。全面分析DeepSeek R1的安全风险，探讨慢思考机制带来的新攻击面和防护策略，为AI安全研究提供参考。"><meta property="og:type" content="article"><meta property="og:title" content="DeepSeek技术原理解读及模型安全风险分析"><meta property="og:url" content="https://mundi-xu.github.io/2025/02/14/Deepseek-Technical-Principle-Explanation-and-Model-Security-Risk-Assessment/index.html"><meta property="og:site_name" content="Hanyin&#39;s Space"><meta property="og:description" content="深度解析DeepSeek V3和R1的核心技术原理，包括MLA架构优化、MoE专家混合、FP8混合精度训练等创新技术。全面分析DeepSeek R1的安全风险，探讨慢思考机制带来的新攻击面和防护策略，为AI安全研究提供参考。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/Benchmark-performance-of-DeepSeek-V3.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/Training-costs-of-DeepSeek-V3.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/basic-architecture-of-DeepSeek-V3.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/kv-cache-optimization.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/attn_variants.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/Comparison-of-the-KV-cache.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/MoE.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/GPT.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/MTP.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/pipeline-parallelism-from-colossal-ai.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/FP8.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/deepseek-r1-performance.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/DeepSeek-R1-Zero.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/DeepSeek-R1-Zero-RL.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/zCoT.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-1.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-2.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-3.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-4.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-5.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-6.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-7.png"><meta property="og:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-8.png"><meta property="article:published_time" content="2025-02-14T12:05:21.000Z"><meta property="article:modified_time" content="2025-02-17T13:05:10.000Z"><meta property="article:author" content="煊宇"><meta property="article:tag" content="DeepSeek"><meta property="article:tag" content="LLM Security"><meta property="article:tag" content="Model Safety"><meta property="article:tag" content="Threat Modeling"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/Benchmark-performance-of-DeepSeek-V3.png"><meta name="format-detection" content="telephone=no"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanyin&#39;s Space"><meta name="referrer" content="no-referrer-when-downgrade"><meta name="renderer" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge"><link rel="apple-touch-icon" sizes="180x180" href="/img/favicon_io/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/img/favicon_io/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/img/favicon_io/favicon-16x16.png"><link rel="manifest" href="/img/favicon_io/site.webmanifest"><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "DeepSeek技术原理解读及模型安全风险分析",
    "author": {
      "@type": "Person",
      "name": "煊宇"
    },
    "datePublished": "2025-02-14T12:05:21.000Z",
    
    "dateModified": "2025-02-17T13:05:10.000Z",
    
    "description": "深度解析DeepSeek V3和R1的核心技术原理，包括MLA架构优化、MoE专家混合、FP8混合精度训练等创新技术。全面分析DeepSeek R1的安全风险，探讨慢思考机制带来的新攻击面和防护策略，为AI安全研究提供参考。",
    
    "publisher": {
      "@type": "Organization",
      "name": "Hanyin&#39;s Space",
      
      "logo": {
        "@type": "ImageObject",
        "url": "/img/favicon_io/favicon.ico"
      }
      
    },
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://mundi-xu.github.io/2025/02/14/Deepseek-Technical-Principle-Explanation-and-Model-Security-Risk-Assessment/index.html"
    },
    
    "keywords": "LLM Security,Threat Modeling,DeepSeek,Model Safety",
    
    
    "articleBody": "DeepSeek V3 &amp;amp; R1关键技术分析 主要思路：  降低训练成本：通过FP8低精度训练、DualPipe双向流水线等 降低推理成本：优化MoE负载均衡等 优化训练数据：使用 14.8T 高质量、多样化的 token，增加了数学和编程样本的比例，扩大了多语言覆盖范围 进一步提升效果：多 Token 预测（MTP）、从 DeepSeek-R1 中蒸馏推理能力等  效果：  在 MMLU、MMLU-Pro、GPQA 等知识性基准测试中，性能与 GPT-4o、Claude-3.5-Sonnet 等领先闭源模型相当。 在 代码和数学 基准测试中，取得了最先进的性能，甚至超越了 GPT-4o。 在 AlpacaEval 2.0 和 Arena-Hard 的开放式评估中表现出色。  训练成本 ：  总成本 ：278.8 万 H800 GPU 小时，约 557.6 万美元。 预训练效率 ：每训练 1 万亿个 token 仅需 18 万 H800 GPU 小时，训练过程稳定，无需回滚。  开源情况：  技术报告：DeepSeek-V3 Technical Report 权重（大小足有67"
    
  }</script><title>DeepSeek技术原理解读及模型安全风险分析 - Hanyin&#39;s Space</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/hint.css/3.0.0/hint.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lxgw-wenkai-screen-webfont/1.7.0/style.min.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"mundi-xu.github.io",root:"/",version:"1.9.8",typing:{enable:!0,typeSpeed:80,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!0,offset_factor:3},web_analytics:{enable:!0,follow_dnt:!1,google:{measurement_id:"G-3847WCVNF2"}},search_path:"/local-search.json",include_content_in_search:!0};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=G-3847WCVNF2",function(){function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","G-3847WCVNF2")})</script><meta name="generator" content="Hexo 8.0.0"><link rel="alternate" href="/atom.xml" title="Hanyin's Space" type="application/atom+xml"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Hanyin&#39;s Space</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>Home</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>Archives</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>Categories</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>Tags</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>About</span></a></li><li class="nav-item"><a class="nav-link" href="/links/" target="_self"><i class="iconfont icon-link-fill"></i> <span>Links</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" rel="external nofollow noreferrer" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" rel="external nofollow noreferrer" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/banner.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="DeepSeek技术原理解读及模型安全风险分析"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-02-14 20:05" pubdate>February 14, 2025 pm</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 8.6k words </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 72 mins </span><span id="busuanzi_container_page_pv" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="busuanzi_value_page_pv"></span> views</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"><div class="category row nomargin-x"><a class="category-item list-group-item category-item-action col-10 col-md-11 col-xm-11" title="LLM Security" id="heading-4d391193c1872e51ad0fd06f40a6e7b3" role="tab" data-toggle="collapse" href="#collapse-4d391193c1872e51ad0fd06f40a6e7b3" aria-expanded="true">LLM Security <span class="list-group-count">(5)</span> <i class="iconfont icon-arrowright"></i></a><div class="category-collapse collapse show" id="collapse-4d391193c1872e51ad0fd06f40a6e7b3" role="tabpanel" aria-labelledby="heading-4d391193c1872e51ad0fd06f40a6e7b3"><div class="category-post-list"><a href="/2025/09/11/getting-started-with-llm-security/" title="大模型安全入门：从零构建你的 AI 安全攻防知识体系" class="list-group-item list-group-item-action"><span class="category-post">大模型安全入门：从零构建你的 AI 安全攻防知识体系</span> </a><a href="/2025/09/10/ai-agent-trust-chain-failure/" title="AI Agent 的信任链是如何断裂的" class="list-group-item list-group-item-action"><span class="category-post">AI Agent 的信任链是如何断裂的</span> </a><a href="/2025/02/14/Deepseek-Technical-Principle-Explanation-and-Model-Security-Risk-Assessment/" title="DeepSeek技术原理解读及模型安全风险分析" class="list-group-item list-group-item-action active"><span class="category-post">DeepSeek技术原理解读及模型安全风险分析</span> </a><a href="/2024/12/18/AI-Insights-2024/" title="AI安全风险洞察：2024" class="list-group-item list-group-item-action"><span class="category-post">AI安全风险洞察：2024</span> </a><a href="/2023/07/26/Security-Risk-Analysis-of-Huawei-Mindspore/" title="MindSpore风险剖析与测试指南" class="list-group-item list-group-item-action"><span class="category-post">MindSpore风险剖析与测试指南</span></a></div></div></div></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">DeepSeek技术原理解读及模型安全风险分析</h1><p id="updated-time" class="note note-primary" style="display:none">Last updated on 2025-02-17T21:05:10+08:00</p><div class="markdown-body"><h1 id="deepseek-v3-r1关键技术分析">DeepSeek V3 &amp; R1关键技术分析</h1><p><strong>主要思路：</strong></p><ul><li>降低训练成本：通过FP8低精度训练、DualPipe双向流水线等</li><li>降低推理成本：优化MoE负载均衡等</li><li>优化训练数据：使用 14.8T 高质量、多样化的 token，增加了数学和编程样本的比例，扩大了多语言覆盖范围</li><li>进一步提升效果：多 Token 预测（MTP）、从 DeepSeek-R1 中蒸馏推理能力等</li></ul><p><strong>效果：</strong></p><ul><li>在 MMLU、MMLU-Pro、GPQA 等知识性基准测试中，性能与 GPT-4o、Claude-3.5-Sonnet 等领先闭源模型相当。</li><li>在 代码和数学 基准测试中，取得了最先进的性能，甚至超越了 GPT-4o。</li><li>在 AlpacaEval 2.0 和 Arena-Hard 的开放式评估中表现出色。</li></ul><p><strong>训练成本 ：</strong></p><ul><li>总成本 ：278.8 万 H800 GPU 小时，约 557.6 万美元。</li><li>预训练效率 ：每训练 1 万亿个 token 仅需 18 万 H800 GPU 小时，训练过程稳定，无需回滚。</li></ul><p><strong>开源情况：</strong></p><ul><li>技术报告：<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">DeepSeek-V3 Technical Report</a></li><li>权重（大小足有671B，FP8精度）：<a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base">deepseek-ai/DeepSeek-V3-Base · Hugging Face</a></li></ul><figure><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/Benchmark-performance-of-DeepSeek-V3.png" alt="Benchmark performance of DeepSeek-V3 and its counterparts"><figcaption aria-hidden="true">Benchmark performance of DeepSeek-V3 and its counterparts</figcaption></figure><h2 id="核心降成本">核心：降成本</h2><p>模型效果好 训练过程快 推理成本低，相比同等性能开源模型训练成本成倍降低</p><figure><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/Training-costs-of-DeepSeek-V3.png" alt="Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour"><figcaption aria-hidden="true">Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour</figcaption></figure><h3 id="模型结构优化">模型结构优化</h3><ol type="1"><li>MLA技术，降低计算过程中的K, V Cache，降低成本。</li><li>DeepSeek MoE，更多专家模型，总共671B参数，激活37B（相当于小模型的激活量），提高推理效率。</li></ol><h3 id="训练优化">训练优化</h3><ol type="1"><li>加入MTP多token预测模块，提高训练效率。</li><li>二阶段上下文长度扩展 4K-&gt;32K，32K-&gt;128K。通过在预训练的时候首先去在一个短的上下文上去训练一个基础的一个模型，再经过微调去扩展到一个比较长的一个上下文，减少训练时间。</li><li>自研大模型训练加速框架HAI-LLM，融合多项性能优化工程技巧，在超大规模训练任务中首次使用FP8混合精度提升训练效率</li></ol><h3 id="通信优化">通信优化</h3><ol type="1"><li>DualPipe算法减少bubble</li><li>ALL2ALL通信和计算掩盖</li></ol><h3 id="内存优化">内存优化</h3><ol type="1"><li>重采样RMSNorm和MLA上采样，以算换存</li><li>将EMA权重存储在CPU内存，异步更新</li></ol><h2 id="架构创新">架构创新</h2><p>DeepSeek<strong>在模型主框架上与主流LLM模型并无差异</strong>，主要创新点集中在<strong>Transformer</strong>块。差异点在于：</p><ul><li><strong>提出MLA结构</strong>，改进Attention计算方式，缩小KV Cache缓存，提高推理速度。</li><li><strong>提出DeepSeek MoE架构</strong>，激活部分参数，降低推理成本，提高推理速度。</li></ul><figure><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/basic-architecture-of-DeepSeek-V3.png" alt="Illustration of the basic architecture of DeepSeek-V3"><figcaption aria-hidden="true">Illustration of the basic architecture of DeepSeek-V3</figcaption></figure><h3 id="multi-head-latent-attention">Multi-Head Latent Attention</h3><blockquote><p>MLA技术：MLA继承自DeepSeek V2的MLA架构，通过将多头注意力的Key和Value映射到低维共享潜在向量空间，实现动态压缩KV缓存，替代传统的逐头存储方式，且并不会导致明显的性能下降。</p></blockquote><ol type="1"><li><p>原始Attention的缺点：</p><ul><li>每次计算Attention时都需要重新计算键值对，导致大量重复计算。</li><li><strong>显著增加计算开销</strong>，降低推理效率。</li></ul></li><li><p>使用KV Cache的原因：</p><ul><li>KV Cache用于存储计算Attention时的键值对，<strong>避免重复计算</strong>。</li><li>支持高效的自回归生成，提升推理性能。</li></ul></li><li><p>减少KV Cache的目的：</p><ul><li>在更少的设备上<strong>处理更长的上下文</strong>。</li><li>提升推理速度和吞吐量，<strong>降低推理成本</strong>。</li></ul></li><li><p>KV Cache的挑战：</p><ul><li>KV Cache随输入长度动态增长，可能超出单卡或多卡显存限制。</li><li>跨设备通信带宽较低，影响性能，因此需尽量减少跨设备部署。</li></ul></li></ol><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/kv-cache-optimization.png"></p><p><strong>为什么降低KV Cache的大小如此重要？</strong></p><p>众所周知，一般情况下LLM的推理都是在GPU上进行，单张GPU的显存是有限的，一部分我们要用来存放模型的参数和前向计算的激活值，这部分依赖于模型的体量，选定模型后它就是个常数；另外一部分我们要用来存放模型的KV Cache，这部分不仅依赖于模型的体量，还依赖于模型的输入长度，也就是在推理过程中是动态增长的，当Context长度足够长时，它的大小就会占主导地位，可能超出一张卡甚至一台机（8张卡）的总显存量。</p><p>在GPU上部署模型的原则是：能一张卡部署的，就不要跨多张卡；能一台机部署的，就不要跨多台机。这是因为“卡内通信带宽 &gt; 卡间通信带宽 &gt; 机间通信带宽”，由于“木桶效应”，模型部署时跨的设备越多，受设备间通信带宽的的“拖累”就越大，事实上即便是单卡H100内SRAM与HBM的带宽已经达到了3TB/s，但对于Short Context来说这个速度依然还是推理的瓶颈，更不用说更慢的卡间、机间通信了。</p><p>所以，减少KV Cache的目的就是要实现在更少的设备上推理更长的Context，或者在相同的Context长度下让推理的batch size更大，从而实现更快的推理速度或者更大的吞吐总量。当然，最终目的都是为了实现更低的推理成本。</p><p>MLA架构中KV<strong>共享同一个存储张量</strong>，且引入低秩投影，有效减少KV Cache（下图中<strong>仅阴影部分需要存储</strong>）。<strong>用计算换存储</strong>，引入额外的计算量，但相比存储消耗，收益更大。</p><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/attn_variants.png"></p><ul><li><p>MQA代表模型：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.02311">PaLM</a>、<a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.06161">StarCoder</a>、<a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.11805">Gemini</a></p></li><li><p>GQA代表模型：LLAMA2,3，Qwen2，ChatGLM</p></li></ul><p>MLA架构：1）分别对Query、Key-Value pair进行低秩压缩；2）使用RoPE获得位置信息；3）使用MHA计算得到输出。</p><figure><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/Comparison-of-the-KV-cache.png" alt="Comparison of the KV cache per token among different attention mechanisms"><figcaption aria-hidden="true">Comparison of the KV cache per token among different attention mechanisms</figcaption></figure><p>对DeepSeekv3而言，<span class="math inline"><em>n</em><sub><em>h</em></sub> = 128</span>，MLA可以将KV Cache降低为 <span class="math inline">$\frac{\frac{9}{2}}{2n_h}=1.7\%$</span></p><h3 id="deepseek-moe">DeepSeek MoE</h3><blockquote><p>DeepSeek MoE技术：DeepseekMoE通过<strong>精细分割专家</strong>、<strong>引入共享专家</strong>和<strong>优化路由选择</strong>，解决了传统MoE中<strong>专家知识重叠和负载不均衡</strong>的问题。具体包括：将专家细分为更多小专家以增强知识分解能力，隔离共享专家以捕获通用知识，并通过专家级和设备级平衡损失优化路由选择，避免路由崩溃和计算瓶颈，从而提升模型在处理复杂任务时的效率和准确性。</p></blockquote><ul><li>Dense模型：对所有输入使用<strong>全部参数</strong>进行计算，计算成本高但实现简单。</li><li>MoE模型：通过路由机制动态激活<strong>部分专家网络的参数</strong>进行计算，降低了<strong>计算成本</strong>，同时支持扩展模型规模以提升性能。</li></ul><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/MoE.png"></p><p><strong>精细分割专家</strong>:</p><ol type="1"><li>增强知识分解能力：细分为多个小专家，使每个专家专注于更细粒度的任务，提升<strong>专家专业化水平</strong>。</li><li>提高组合灵活性：激活专家组合的<strong>灵活性显著增强</strong>，可动态选择更合适的专家组合，<strong>提升任务处理能力</strong>。</li></ol><p><strong>引入共享专家</strong>：</p><ol type="1"><li>学习通用知识：共享专家专门用于<strong>学习通用知识</strong>，避免其他路由专家重复学习通用知识，减少参数冗余。</li><li>提升参数效率：通过隔离共享专家，路由专家可以更专注于<strong>学习独特知识</strong>，提高<strong>模型参数利用效率</strong>。</li></ol><p><strong>优化路由选择</strong>：</p><ol type="1"><li>避免路由崩溃：确保每个专家都能获得足够的训练机会，<strong>避免模型总是选择少数专家而忽略其他专家</strong>。</li><li>缓解计算瓶颈：确保不同设备上的专家负载均衡，避免计算资源浪费和瓶颈问题，提高分布式计算的效率。</li></ol><h2 id="训练方法创新">训练方法创新</h2><h3 id="multi-token-prediction">Multi-Token Prediction</h3><blockquote><p>DeepSeek MTP技术：大语言模型传统上采用单个token预测训练方式，即每次只预测下一个token。 DeepSeek V3基于META提出的多token预测方法，进行了改进，采用<strong>链式结构</strong>而非并行结构，同时保持完整的因果链。这种改进既保留了多token预测的优势，又通过维持因果关系来提升预测质量。不仅提升了模型性能，还<strong>改善了模型的泛化能力</strong>。</p></blockquote><p><strong>传统大模型采用自回归方式逐token预测</strong>:</p><ul><li><strong>训练效率低</strong>：每次在生成一个token的时候，都要频繁跟访存交互，加载KV-Cache，再通过多层网络做完整的前向计算。对于这样的访存密集型的任务，通常会因为访存效率形成训练或推理的瓶颈。</li><li><strong>长文本建模能力弱</strong>：一次只学习单个token，上下文依赖弱，容易陷入局部最优解。</li></ul><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/GPT.png"></p><p><strong>DeepSeek V3 MTP</strong>:</p><ul><li>主网络结构中<strong>接入2个预测头</strong>，针对输入token <span class="math inline"><em>t</em><sub><em>i</em></sub></span>分别预估后续的<span class="math inline"><em>t</em><sub><em>i</em> + 1</sub></span>， <span class="math inline"><em>t</em><sub><em>i</em> + 2</sub></span></li><li>预测头之间是<strong>串行架构</strong>，预测第 <span class="math inline"><em>i</em> + 2</span> 个token时，会把第 <span class="math inline"><em>i</em> + 1</span> 个token也作为输入，保证完整的序列推理链实现串行预测</li><li>一次预测多个token，有效提升训练性能，次token的接受率稳定在85%＋，<strong>训练时推理</strong>速度提升1.8倍</li><li>共享Embedding层和输出头<strong>减少内存开销</strong></li><li>MTP能够增强有监督训练信号，帮助模型预先规划对于token的组织和表达，提高<strong>泛化</strong>能力</li></ul><figure><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/MTP.png" alt="Illustration of our Multi-Token Prediction (MTP) implementation"><figcaption aria-hidden="true">Illustration of our Multi-Token Prediction (MTP) implementation</figcaption></figure><h3 id="dualpipe-and-computation-communication-overlap">DualPipe and Computation-Communication Overlap</h3><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/pipeline-parallelism-from-colossal-ai.png"></p><p><strong>当前问题</strong></p><p>当模型规模特别大时，通常需要将其拆分为多个子模块，并分配到多个计算设备上进行并行计算。在此过程中，设备之间需要进行数据通信。当一个设备完成其计算任务后，必须将结果传输给下一个设备，以便后续计算任务能够继续执行。然而，这种数据通信过程会导致<strong>部分设备处于空闲状态</strong>，从而造成计算<strong>资源的浪费</strong>。</p><p><strong>DeepSeek解决方案</strong></p><ol type="1"><li><strong>更细分工</strong>：DualPipe把每个GPU的任务分得更细，比如让一个GPU同时负责模型的开头和结尾部分。这样，GPU之间可以同时干活，不用总是等着别人。</li><li><strong>双向流水线</strong>：普通的流水线是单向的，比如数据从GPU 1传到GPU 2，再传到GPU 3。DualPipe让数据从两头同时传，比如GPU 1和GPU 8同时开始干活，这样中间的GPU也能更忙起来，减少了等待时间。</li><li><strong>优化通信</strong>：DualPipe还改进了GPU之间的通信方式，让数据传输更快，减少了通信占用的时间。</li></ol><h3 id="fp8混合精度训练">FP8混合精度训练</h3><figure><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/FP8.png" alt="The overall mixed precision framework with FP8 data format"><figcaption aria-hidden="true">The overall mixed precision framework with FP8 data format</figcaption></figure><table><thead><tr><th>格式</th><th>位数</th><th>精度</th><th>动态范围</th><th>计算速度</th><th>内存占用</th><th>适用场景</th></tr></thead><tbody><tr><td>FP32</td><td>32 位</td><td>高精度</td><td>非常大</td><td>慢</td><td>大</td><td>传统高精度计算 (如科学计算)</td></tr><tr><td>BF16</td><td>16 位</td><td>中等精度</td><td>较大</td><td>较快</td><td>中等</td><td>深度学习训练 (兼顾精度和效率)</td></tr><tr><td>FP8</td><td>8 位</td><td>低精度</td><td>较小</td><td>非常快</td><td>小</td><td>低精度训练 (追求极致效率)</td></tr></tbody></table><p><strong>当前问题</strong></p><p>训练大模型太贵了！FP8低精度训练可以大幅减少计算和内存开销，但直接使用 FP8 会导致数值不稳定，模型训练可能失败。因此，需要找到一种方法，既能享受 FP8 的高效，又能避免它的缺点。</p><p><strong>DeepSeek解决方案</strong></p><ol type="1"><li><strong>精度解耦</strong>：把模型的不同部分分开处理，对不敏感的部分用 FP8，对敏感的部分保持高精度（如 BF16 或 FP32）。</li><li><strong>自动缩放</strong>：动态调整数据的缩放比例，确保数值在 FP8 的范围内，避免溢出或精度丢失。</li><li><strong>细粒度量化</strong>：对数据进行分组缩放，比如每 128 个通道一组，既保证精度又提高效率。</li><li><strong>递增累加精度</strong>：在计算过程中，先用 FP8 快速计算，隔一段时间再用高精度（FP32）累加结果，减少误差积累。</li></ol><h2 id="deepseek-r1训练过程">DeepSeek R1训练过程</h2><p>DeepSeek-R1 在<strong>推理任务</strong>上实现了与 OpenAI-o1-1217 相当的性能。 DeepSeek-R1以 <strong>DeepSeek-V3-Base(671B)</strong> 为基础模型，使用<strong>GRPO算法</strong>作为RL框架来提升Reasoning性能。开源发布了6个基于<strong>DeepSeek-R1</strong>蒸馏的更小稠密模型（ Qwen/Llama 1.5B, 7B, 8B, 14B, 32B, 70 ）</p><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/deepseek-r1-performance.png"></p><p>DeepSeek V3, R1和R1-Zero区别：</p><ul><li>R1-Zero 基于 DeepSeek-V3-Base，通过 RL （强化学习） 训练，无 STF （监督微调），具备AI自我进化范式。</li><li>R1 则基于 R1-Zero，增加STF（监督微调），先利用少量人工标注的高质量数据进行冷启动微调，再进行 RL。</li></ul><p>关键技术点：</p><ol type="1"><li>DeepSeek-R1-Zero直接基于V3 Base做RL，不依赖SFT初始化，模型依然能自己学习到推理能力。</li><li>奖励模型是基于规则的，Accuracy rewards（答案的正确性）和Format rewards（强制思考过程在<code>&lt;think&gt;&lt;/think&gt;</code>之间）</li><li>提出了一种提高模型推理能力的训练流程，可生成高质量推理数据。</li></ol><h3 id="deepseek-r1-zero">DeepSeek R1-Zero</h3><blockquote><p>DeepSeek R1-Zero 训练核心思路：1. 不做监督微调 2. 强化学习中放弃过程性奖励，直接根据最终结果及输出格式作为奖励函数</p></blockquote><pre>
<code class="mermaid">
flowchart LR
    A["DeepSeek-V3-Base"] --> B["强化学习（GRPO）<br>规则奖励函数"]
    B --> C["DeepSeek-R1-Zero"]
</code>
</pre><ul><li>正确性奖励：评估response是否正确（数学，代码，逻辑）<br>比如带有确定结果的数学问题，模型需要提供指定格式的最终答案，来增强基于规格的判别正确性。比如对于leedcode问题，针对预设的测试用例可以通过编译器生成反馈信号。</li><li>格式奖励：评估输出格式是否符合要求<br>另外还采用了基于格式的奖励，强制模型将思考过程放在<code>&lt;think&gt; &lt;/think&gt;</code>标签之间。</li></ul><p>训练模板：推理过程和答案包裹在标签里面的形式 <code>&lt;think&gt; reasoning process here &lt;/think&gt;&lt;answer&gt; answer here &lt;/answer&gt;</code></p><figure><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/DeepSeek-R1-Zero.png" alt="Template for DeepSeek-R1-Zero"><figcaption aria-hidden="true">Template for DeepSeek-R1-Zero</figcaption></figure><p>随着RL的训练进行，模型的输出逐渐变长，逐渐学习推理能力 <img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/DeepSeek-R1-Zero-RL.png" alt="The average response length of DeepSeek-R1-Zero on the training set during the RL process."></p><p>最终结果：推理能力提升，但回答<strong>格式混乱、语言混杂</strong></p><h3 id="deepseek-r1">DeepSeek R1</h3><blockquote><p>DeepSeek R1训练核心思路：1. 通过SFT+RL等训练方式获得可用模型构造高质量数据集 2. 利用高质量数据集遵照V3训练pipeline对V3 base模型做SFT和RL训练，得到R1模型。</p></blockquote><p>DeepSeek-R1训练过程分为<strong>两阶段四个步骤</strong>，目标：</p><ol type="1"><li>通过少量高质量数据作为冷启动，提升推理能力和加速收敛</li><li>训练一个用户友好的模型，使其产生清晰连贯的思维链CoT，还能表现出强大的通用能力</li></ol><h4 id="第一阶段训练出一个可用模型生成高质量数据集">第一阶段：训练出一个可用模型生成高质量数据集</h4><h5 id="冷启动sft约几千条">冷启动SFT（约几千条）</h5><p><strong>数据集</strong>：</p><ul><li>Few-shot：带有long cot的例子作为few shot，引导模型生成回答。（V3-Base）</li><li>Zero-shot：直接在prompt中要求模型输出带有思维链的回答。（V3-Base）</li><li>部分DeepSeek-R1-Zero输出</li><li>人工做后处理完善结果</li></ul><p><strong>数据格式</strong>：</p><ul><li>&lt;问题，思考过程，回答&gt;</li></ul><p><strong>微调</strong>：以DeepSeek-V3-Base为基础模型微调</p><p><strong>目的</strong>：训练一个指令性遵从较好的模型。</p><p><strong>模型</strong>：<strong>DeepSeek-R1-SFT-1</strong></p><h5 id="强化学习">强化学习</h5><p><strong>数据集（同R1-Zero）</strong>：</p><ul><li>Math, Code,逻辑推理等…</li></ul><p><strong>数据格式</strong>：</p><ul><li>&lt;问题，回答&gt;</li></ul><p>数据集数量未知</p><p><strong>基于GRPO算法的RL训练</strong>:</p><ul><li>训练奖励函数同R1-Zero一致</li></ul><p>提高模型在具有明确解决方案的问题中的推理能力。</p><p>目的：学习推理能力，训练具有一定推理能力的模型，用于<strong>自动化大规模</strong>生成最终训练的数据集。</p><p>模型：<strong>DeepSeek-R1-RL-1</strong></p><h4 id="第二阶段使用第一阶段高质量数据常规rl训练得到r1模型">第二阶段：使用第一阶段高质量数据+常规RL训练，得到R1模型</h4><h5 id="拒绝采样sft">拒绝采样+SFT</h5><p><strong>收集SFT数据</strong>：只包含问题，不包含答案。</p><p><strong>推理数据</strong>：基于前一阶段 DeepSeek-R1-RL-1 执行拒绝采样生成推理轨迹。每个提示采样多个响应，并保留正确的响应，共收集600K训练样本</p><p><strong>非推理数据</strong>：复用 DeepSeek-V3 的 SFT 数据集的一部分，共收集200K。</p><p>在 DeepSeek-V3 base 模型上用800K样本做 2epoch SFT 训练。</p><p>目的：这个阶段的模型主要是解决 R1-Zero 存在的可读性差和语言混乱的问题。</p><p>模型：<strong>DeepSeek-R1-SFT-2</strong></p><h5 id="全场景强化学习">全场景强化学习</h5><p>目的：这个阶段的RL训练主要是提高模型推理能力。以及进一步对齐人类偏好，提高模型的有用性和无害性。训练过程同V3一致。</p><p>模型：<strong>DeepSeek-R1</strong></p><h2 id="simple-test-time-scaling-1000条sft数据微调实现o1-like推理">Simple test-time scaling: 1000条SFT数据微调实现O1-like推理</h2><p><strong>核心概念</strong>:</p><p>Test-time Scaling是一种在模型推理阶段利用额外计算资源提升性能的技术，其核心思想是通过引入更多计算或复杂策略，使模型在生成答案时<strong>进行更深入的思考或多次验证</strong>，从而提高输出的准确性和可靠性。</p><p><strong>核心贡献</strong>:</p><ol type="1"><li>提出了一种非常简单的Test-time Scaling方式， <strong>Budget Forcing</strong><ul><li><strong>强制结束</strong>：若超过最大token数量，强制结束思考过程，并输出答案。</li><li><strong>延长思考</strong>：若提前结束思考，则添加 <code>Wait token</code> 来鼓励模型进行更多的探索。</li></ul></li><li>构建高质量小规模数据集微调模型，验证方法有效性。</li></ol><p><strong>启发</strong>：</p><ol type="1"><li>大部分模型都有更强的推理潜力，需要被激活</li><li>训练数据质量比数量更重要</li></ol><h2 id="总结">总结</h2><ol type="1"><li>高质量数据对提升模型推理能力至关重要。通过蒸馏大模型数据<strong>构建高质量数据集</strong>，是提升小模型性能的最有效方法之一。</li><li>当前LLM普遍具备更强的潜在推理能力，可通过<strong>Test time Scaling</strong>技术激发。</li><li>模型<strong>内生安全</strong>能力的提升可能<strong>仍需依赖SFT</strong>，因为RL仅在具有明确结果和规则的数据集上表现出良好的推理能力，而内生安全对逻辑性的要求可能相对较低。<ul><li><em>数学、代码等数据，具有高度结构化和明确的逻辑规则，结果通常是确定性的，可以通过形式化方法进行验证。</em></li><li><em>内容安全数据，通常是开放域的、非结构化的（如文本、图像、语音）。涉及主观判断（例如，什么是“有害内容”可能因文化、语境而异）。</em></li></ul></li><li>模型能力越强可能越容易遭受攻击，如海绵样本、越狱等。攻击者还可能通过控制思维过程来操纵模型输出。</li></ol><h1 id="r1模型安全风险分析">R1模型安全风险分析</h1><h2 id="模型安全后门">模型安全（后门）</h2><p>模型安全可使用业界SOTA的<strong>LLM模型后门检测工具 BAIT</strong>（发表于S&amp;P 2025）进行测试，使用DeepSeek-R1生成的推理数据训练出的系列模型暂未发现植入的模型后门。</p><h2 id="生成内容安全越狱隐私">生成内容安全（越狱、隐私）</h2><h3 id="思维链chain-of-thought-cot">思维链/Chain-of Thought (CoT)</h3><p>R1的慢推理其实是思维链发展来的，目前<strong>LLM普遍可以生成思维链</strong>，但<strong>不会主动触发</strong>。需要提示词触发模型生成思维链，思维链内容作为有效信息引导模型给出正确回答。 <img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/zCoT.png"></p><ul><li>CoT的应用（zero-shot or few shot）：提升模型在特定问题上回答的<strong>准确性、规范性，数据生成</strong>等；突破模型安全边界，利用模板、违规问答等方式<strong>诱导模型输出有害内容</strong>。</li><li>CoT的不足：1）用户需要根据问题去<strong>设计prompt</strong>以引导大模型进行reasoning；2）reasoning过程<strong>严重依赖于输入的prompt的优劣</strong>。</li></ul><p>DeepSeek-R1的“慢思考”、Reasoning可以有效提升<strong>内生安全防护</strong>，但<strong>开辟了另外的攻击面</strong></p><h3 id="慢思考有效提升内生安全防护">慢思考有效提升内生安全防护</h3><p>DeepSeek-R1在思维链中可<strong>主动意识</strong>到要保护隐私数据，通过慢思考提醒自己，答案中涉及的隐私信息必须是<strong>随机生成、虚构、测试数据</strong></p><ul><li>能意识到和敏感信息相关的数据应当是“虚构的”“测试数据”</li><li>针对用户对敏感信息的询问，能意识到要“随机生成”答案</li><li>甚至可能意识到“<strong>用户正在测试我是否存在漏洞</strong>”</li></ul><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-1.png"></p><p>DeepSeek-R1在reasoning过程中<strong>对用户的合理需求以及非法需求分别进行了分析</strong>，并得出结论：要在安慰用户的同时不提供非法信息</p><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-2.png"></p><p>对比DeepSeek-V3被越狱成功输出真实的激活码</p><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-3.png"></p><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-4.png"></p><p>但是<strong>慢思考不能完全避免有害回答</strong>，可能出现在思维链中<strong>明确意识到</strong>要避免有害回答，但答案中依然<strong>出现有害回答</strong>。</p><p><strong>根本原因</strong>：<strong>Faithfulness（幻觉的一种）不足</strong>，即没有完全依据思维链生成回答。</p><p><strong>结论：慢思考有助于避免有害回答，但不完全可靠，风控依然是必要的。</strong></p><h3 id="慢思考开辟了另外的攻击面">慢思考开辟了另外的攻击面</h3><p>DeepSeek-R1在思维链用了<code>&lt;think&gt;&lt;answer&gt;</code>等标签，存在标签伪造风险，可植入思考过程、历史答案</p><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-5.png"></p><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-6.png"></p><p>需要警惕提示词中标签的来源，避免思维链伪造,有更多潜在的不安全标签，引入的风险待探索。</p><h2 id="应用安全智能体劫持海绵样本">应用安全（智能体劫持、海绵样本）</h2><p>同时暴露思维链让对抗变更容易，暴露思维链 = 暴露大模型的思考思路 -&gt; 让对抗（越狱、劫持）更<strong>有的放矢</strong>。根本原因是反馈信息从“劫持成功与否”这个二元的反馈变成了整个推理过程，具备了更多信息。</p><p>例如在<a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/2312.02119">Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</a> 中就介绍了TAP: 一种迭代式越狱话术优化方法。同时在Cisco测试报告（<a target="_blank" rel="noopener" href="https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models">Evaluating Security Risk in DeepSeek and Other Frontier Reasoning Models</a>)中表示基于R1反馈信息优化越狱话术，可实现100%攻击成功率。</p><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-7.png"></p><p>此外，DeepSeek在RL训练中放任思维链变长，更容易触发<strong>海绵样本</strong>：</p><ul><li>方式1：“写出尽可能多的xxx”</li><li>方式2：稍微有点复杂的数学问题</li><li>方式3：解释一个矛盾的命题</li></ul><p><img lazyload src="/img/loading.gif" data-src="https://raw.githubusercontent.com/Mundi-Xu/picture_resource/master/picture/DeepSeek/r1-security-8.png"></p><p>大模型易被上文distract，不相干的reasoning内容反倒会削弱模型能力；reasoning过程易陷入死循环、发散时较难停止，因此更容易遭受海绵攻击。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/LLM-Security/" class="category-chain-item">LLM Security</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/LLM-Security/" class="print-no-link">#LLM Security</a> <a href="/tags/Threat-Modeling/" class="print-no-link">#Threat Modeling</a> <a href="/tags/DeepSeek/" class="print-no-link">#DeepSeek</a> <a href="/tags/Model-Safety/" class="print-no-link">#Model Safety</a></div></div><div class="license-box my-3"><div class="license-title"><div>DeepSeek技术原理解读及模型安全风险分析</div><div>https://mundi-xu.github.io/2025/02/14/Deepseek-Technical-Principle-Explanation-and-Model-Security-Risk-Assessment/</div></div><div class="license-meta"><div class="license-meta-item"><div>Author</div><div>煊宇</div></div><div class="license-meta-item license-meta-date"><div>Posted on</div><div>February 14, 2025</div></div><div class="license-meta-item"><div>Licensed under</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/" rel="external nofollow noreferrer"><span class="hint--top hint--rounded" aria-label="BY - Attribution"><i class="iconfont icon-cc-by"></i> </span></a><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/" rel="external nofollow noreferrer"><span class="hint--top hint--rounded" aria-label="NC - Non-commercial"><i class="iconfont icon-cc-nc"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/09/10/ai-agent-trust-chain-failure/" title="AI Agent 的信任链是如何断裂的"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">AI Agent 的信任链是如何断裂的</span> <span class="visible-mobile">Previous</span></a></article><article class="post-next col-6"><a href="/2024/12/18/AI-Insights-2024/" title="AI安全风险洞察：2024"><span class="hidden-mobile">AI安全风险洞察：2024</span> <span class="visible-mobile">Next</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments"><div id="giscus" class="giscus"></div><script type="text/javascript">Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"Mundi-Xu/mundi-xu.github.io","repo-id":"MDEwOlJlcG9zaXRvcnkxNTQ1OTg4Mjg=","category":"Announcements","category-id":"DIC_kwDOCTb9rM4CeUGi","theme-light":"light","theme-dark":"dark","mapping":"title","reactions-enabled":1,"emit-metadata":0,"input-position":"top","lang":"zh-CN","strict":0};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>Table of Contents</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><script>Fluid.utils.createScript("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.8.1/mermaid.min.js",function(){mermaid.initialize({theme:"default"}),Fluid.utils.listenDOMLoaded(function(){Fluid.events.registerRefreshCallback(function(){"mermaid"in window&&mermaid.init()})})})</script><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">Search</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">Keyword</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="mailto:mundi.xu@gmail.com?subject=Interested+In+Your+Blog" rel="external nofollow noreferrer" target="_blank"><span>Contact me</span></a> <i class="iconfont icon-love"></i> <a href="mailto:mundi.xu@gmail.com?subject=Interested+In+Your+Blog" rel="external nofollow noreferrer" target="_blank"><span>mundi.xu@gmail.com</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">Views: <span id="busuanzi_value_site_pv"></span> </span><span id="busuanzi_container_site_uv" style="display:none">Visitors: <span id="busuanzi_value_site_uv"></span></span></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/typed.js/2.1.0/typed.umd.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>var relativeDate=function(){var t,e,a,d,i=document.getElementById("updated-time");i&&(e=/\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/,(a=(t=i.textContent).match(e))&&(d=dayjs(a[0]).fromNow(),i.textContent=t.replace(e,d)),i.style.display="")};Fluid.utils.createScript("https://cdnjs.cloudflare.com/ajax/libs/dayjs/1.11.13/dayjs.min.js",function(){Fluid.utils.createScript("https://cdnjs.cloudflare.com/ajax/libs/dayjs/1.11.13/plugin/relativeTime.min.js",function(){dayjs.extend(dayjs_plugin_relativeTime),"en".startsWith("en")?relativeDate():Fluid.utils.createScript("https://cdnjs.cloudflare.com/ajax/libs/dayjs/1.11.13/locale/en.min.js",function(){dayjs.locale("en"),relativeDate()})})})</script><script>Fluid.utils.createScript("https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.36.4/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://cdnjs.cloudflare.com/ajax/libs/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="/js/DynamicLine.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog works best with JavaScript enabled</div></noscript></body></html>